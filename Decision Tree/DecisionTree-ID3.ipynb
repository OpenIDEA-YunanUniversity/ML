{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以信息增益为特征选择的决策树算法（ID3）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建数据\n",
    "数据来自西瓜书"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">     色泽,根蒂,敲声,纹理,脐部,触感,好瓜\n",
    "- 青绿,蜷缩,浊响,清晰,凹陷,硬滑,是\n",
    "- 乌黑,蜷缩,沉闷,清晰,凹陷,硬滑,是\n",
    "- 乌黑,蜷缩,浊响,清晰,凹陷,硬滑,是\n",
    "- 青绿,蜷缩,沉闷,清晰,凹陷,硬滑,是\n",
    "- 浅白,蜷缩,浊响,清晰,凹陷,硬滑,是\n",
    "- 青绿,稍蜷,浊响,清晰,稍凹,软粘,是\n",
    "- 乌黑,稍蜷,浊响,稍糊,稍凹,软粘,是\n",
    "- 乌黑,稍蜷,浊响,清晰,稍凹,硬滑,是\n",
    "- 乌黑,稍蜷,沉闷,稍糊,稍凹,硬滑,否\n",
    "- 青绿,硬挺,清脆,清晰,平坦,软粘,否\n",
    "- 浅白,硬挺,清脆,模糊,平坦,硬滑,否\n",
    "- 浅白,蜷缩,浊响,模糊,平坦,软粘,否\n",
    "- 青绿,稍蜷,浊响,稍糊,凹陷,硬滑,否\n",
    "- 浅白,稍蜷,沉闷,稍糊,凹陷,硬滑,否\n",
    "- 乌黑,稍蜷,浊响,清晰,稍凹,软粘,否\n",
    "- 浅白,蜷缩,浊响,模糊,平坦,硬滑,否\n",
    "- 青绿,蜷缩,沉闷,稍糊,稍凹,硬滑,否\n",
    "\n",
    "1. 色泽>青绿:1, 乌黑:2, 浅白:3 \n",
    "2. 根蒂>蜷缩:1, 稍蜷:2, 硬挺:3\n",
    "3. 敲声>浊响:1,沉闷:2,清脆:3\n",
    "4. 纹理>清晰:1，稍糊:2, 模糊:3\n",
    "5. 脐部>凹陷:1,稍凹:2,平坦:3\n",
    "6. 触感>硬滑:1,软粘:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "chn_data = [['青绿','蜷缩','浊响','清晰','凹陷','硬滑','是'],\n",
    "['乌黑','蜷缩','沉闷','清晰','凹陷','硬滑','是'],\n",
    "['乌黑','蜷缩','浊响','清晰','凹陷','硬滑','是'],\n",
    "['青绿','蜷缩','沉闷','清晰','凹陷','硬滑','是'],\n",
    "['浅白','蜷缩','浊响','清晰','凹陷','硬滑','是'],\n",
    "['青绿','稍蜷','浊响','清晰','稍凹','软粘','是'],\n",
    "['乌黑','稍蜷','浊响','稍糊','稍凹','软粘','是'],\n",
    "['乌黑','稍蜷','浊响','清晰','稍凹','硬滑','是'],\n",
    "['乌黑','稍蜷','沉闷','稍糊','稍凹','硬滑','否'],\n",
    "['青绿','硬挺','清脆','清晰','平坦','软粘','否'],\n",
    "['浅白','硬挺','清脆','模糊','平坦','硬滑','否'],\n",
    "['浅白','蜷缩','浊响','模糊','平坦','软粘','否'],\n",
    "['青绿','稍蜷','浊响','稍糊','凹陷','硬滑','否'],\n",
    "['浅白','稍蜷','沉闷','稍糊','凹陷','硬滑','否'],\n",
    "['乌黑','稍蜷','浊响','清晰','稍凹','软粘','否'],\n",
    "['浅白','蜷缩','浊响','模糊','平坦','硬滑','否'],\n",
    "['青绿','蜷缩','沉闷','稍糊','稍凹','硬滑','否']]\n",
    "\n",
    "hash_dict =  {'青绿':1,'乌黑':2, '浅白':3, \n",
    "            '蜷缩':1, '稍蜷':2, '硬挺':3,\n",
    "            '浊响':1,'沉闷':2,'清脆':3,\n",
    "            '清晰':1,'稍糊':2, '模糊':3,\n",
    "            '凹陷':1,'稍凹':2,'平坦':3,\n",
    "            '硬滑':1,'软粘':2,\n",
    "              '是':1,'否':0\n",
    "             }\n",
    "\n",
    "train = []\n",
    "for row_vec in chn_data:\n",
    "    tmp = []\n",
    "    for col in row_vec:\n",
    "        tmp.append(hash_dict[col])\n",
    "    train.append(tmp)\n",
    "\n",
    "feature_map = {0:'色泽',1:'根蒂',2:'敲声',3:'纹理',4:'脐部',5:'触感'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法推导 （ID3）\n",
    "### Descision Tree through ID3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "熵(entropy) 定义为：\n",
    "$$P(X=x_i)=P_i, i\\in{1,2,3...n},其中 1,2,3...为特征X_i的取值$$\n",
    "\n",
    "随机变量X的熵定义为（表示这个随机变量的不确定性）\n",
    "$$H(x)=-\\sum_{i=1}^n{P_i*log{P_i}}$$\n",
    "\n",
    "熵只依赖于$X$的分布,而与X的取值无关，so，可以写成\n",
    "$$H(p)=-\\sum_{i=1}^n{P_i*log{P_i}}$$\n",
    "\n",
    "显然，熵值越大，随机变量(特征)的不确定性就越大，当然，在整个数据集中，单独看一个特征的熵是没有意义的，整个训练必定有标签$y$,那么就有$X,Y$的联合概率分布为：\n",
    "$$P(X=x_i,Y=y_j)=P_{ij},    i\\in{1,2,3...n},j\\in{1,2,3..m}$$\n",
    "\n",
    "如上，我们引入条件熵$H(Y|X)$,即表示已知随机变量$X$的条件下，$Y$的不确定性，换句话说，就是$X$对$Y$的某种影响有多大，这里的某种影响就是我们选择特征的重要来源之一：\n",
    "$$H(Y|X)=\\sum_{i=1}^n{P_i*H(Y|X=x_i)}$$\n",
    "\n",
    "那么转换到实际的场景中，我们如何选择特征？=》信息增益，通俗的来讲，信息增益定义为：**已知$X$的信息使得类别$Y$不确定性减少的程度**，也就是我选择那个特征（$X$），能最大限度的分出类别$Y$，这就是信息增益要做的事情，那么信息增益定义为：\n",
    "\n",
    "$$g(D,A)=H(D)-H(D|A)$$\n",
    "\n",
    "其中$g(D,A)$表示在特征$A$下，$A$对整个数据集所产生的信息增益，$H(D)$为整个数据集的熵。\n",
    "\n",
    "那么接下来可以计算数据集$D$的经验熵$H(D)$:\n",
    "\n",
    "$$H(D)=-\\sum_{k=1}^K{\\frac{|C_k|}{|C|}}*{log\\frac{|C_k|}{|C|}}$$\n",
    "\n",
    "其中，$|D|$为样本总量，$|C_k|$为类别总量，接下来计算其条件经验熵(emprical entropy) $H(D|A)$:\n",
    "\n",
    "$$H(D|A)=\\sum_{i=1}^n{\\frac{|D_i|}{|D|}}*H(D_i)=-\\sum_{i=1}^n{\\frac{|D_i|}{|D|}}*\\sum_{k=1}^K\\frac{|D_{ik}|}{|D_i|}*log{\\frac{|D_{ik}|}{|D_i|}}$$\n",
    "\n",
    "其中，$|D_i|$为对应的$A$特征下，取值为$i$的样本数，同理，$|D_{ik}|$就是在$D_i$的子数据下的分类类别，由此信息增益(info gain)可计算：\n",
    "\n",
    "$$g(D,A)=H(D)-H(D|A)$$\n",
    "\n",
    "NOTES:对数方面，一般以2为底，或取自然对数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(vector):\n",
    "    \"\"\"\n",
    "    vector just contain the lables, 1D aray like so\n",
    "    \"\"\"\n",
    "    elements = np.unique(vector)\n",
    "    entropy = 0.0\n",
    "    vec_size = vector.shape[0]\n",
    "    \n",
    "    for unique_val in elements:\n",
    "        prob = list(vector).count(unique_val)/vec_size+0.0\n",
    "        entropy -= prob*np.log2(prob)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(complete_data, feature_idx, val):\n",
    "    \"\"\"\n",
    "    split the original data through feature index and its value\n",
    "    then reurn the subset within label\n",
    "    \"\"\"\n",
    "    subset = np.c_[complete_data[:,feature_idx], complete_data[:,-1]]\n",
    "    subset_target = np.where(subset[:,0]==val)\n",
    "    subset = complete_data[subset_target,:]\n",
    "    subset = subset[0]\n",
    "    return np.delete(subset, feature_idx, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(dataset, feature_idx):\n",
    "    \"\"\"\n",
    "    calculate the appointed features's informathon gain while the dataset\n",
    "    should store all the label, definitely.Otherwise,the programe will be wrong\n",
    "    \"\"\"\n",
    "    base_entropy = calculate_entropy(dataset[:,-1])\n",
    "    empircal_condition_entropy = 0.0\n",
    "    \n",
    "    for feature_val in np.unique(dataset[:, feature_idx]):\n",
    "        subset = split_data(dataset, feature_idx, feature_val)\n",
    "        current_entropy = calculate_entropy(subset[:,-1])\n",
    "        empircal_condition_entropy += (subset.shape[0]/dataset.shape[0]+0.0)*current_entropy\n",
    "    \n",
    "    return base_entropy-empircal_condition_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chose_best_feature(dataset):\n",
    "    maximum_infor_gain = 0.0\n",
    "    best_feature_idx = -1\n",
    "    \n",
    "    for feature_idx in range(dataset.shape[1]-1):\n",
    "        this_infor_gain = info_gain(dataset, feature_idx)\n",
    "        if this_infor_gain>maximum_infor_gain:\n",
    "            best_feature_idx = feature_idx\n",
    "            maximum_infor_gain = this_infor_gain\n",
    "    \n",
    "    return best_feature_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vote(classlist):\n",
    "    return classlist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(train, feature_vec):\n",
    "    \n",
    "    classlist = list(train[:,-1])\n",
    "    if classlist.count(classlist[0]) == len(classlist):\n",
    "        return classlist[0]\n",
    "    if train.shape[0]==1:\n",
    "        \n",
    "        return vote(classlist)\n",
    "    \n",
    "    best_feature_idx = chose_best_feature(train)\n",
    "    best_feature_name = feature_vec[best_feature_idx]\n",
    "    tree = {best_feature_name:{}}\n",
    "\n",
    "    del(feature_vec[best_feature_idx])\n",
    "    \n",
    "    for feature_val in np.unique(train[:,best_feature_idx]):\n",
    "        sub_feature_vec = feature_vec[:]\n",
    "        tree[best_feature_name][feature_val] = build_tree(split_data(train, best_feature_idx, feature_val), sub_feature_vec)\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'纹理': {1: {'根蒂': {1: 1, 2: {'色泽': {1: 1, 2: {'触感': {1: 1, 2: 0}}}}, 3: 0}},\n",
       "  2: {'触感': {1: 0, 2: 1}},\n",
       "  3: 0}}"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vec = ['色泽','根蒂','敲声','纹理','脐部','触感']\n",
    "build_tree(train, feature_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
